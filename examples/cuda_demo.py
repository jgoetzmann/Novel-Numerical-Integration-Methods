#!/usr/bin/env python3
"""
CUDA Demo Script for Novel Numerical Integration Methods

This script demonstrates the CUDA capabilities of the numerical integration methods.
"""

import torch
import numpy as np
import time
import sys
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

from src.core.butcher_tables import ButcherTable, get_rk4
from src.core.integrator_runner import RungeKuttaIntegrator
from src.models.model import MLPipeline, ModelConfig

def demo_cuda_acceleration():
    """Demonstrate CUDA acceleration benefits."""
    print("CUDA Acceleration Demo")
    print("=" * 50)
    
    # Check CUDA availability
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available on this system")
        print("This demo requires CUDA-capable hardware and PyTorch with CUDA support")
        return
    
    print(f"‚úÖ CUDA available: {torch.cuda.get_device_name()}")
    print(f"‚úÖ CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print()
    
    # Demo 1: Neural Network Training
    print("Demo 1: Neural Network Training Acceleration")
    print("-" * 50)
    
    config = ModelConfig()
    ml_pipeline = MLPipeline(config)
    
    # Generate test data
    print("Generating test Butcher tables...")
    tables = ml_pipeline.generator.generate_valid_tables(100, device=str(ml_pipeline.device))
    print(f"Generated {len(tables)} tables")
    
    if len(tables) > 0:
        # Test tensor operations
        print("Testing tensor operations...")
        start_time = time.time()
        
        tensors = [table.to_tensor(device=ml_pipeline.device) for table in tables[:10]]
        tensor_stack = torch.stack(tensors)
        
        # Simulate some computation
        result = torch.matmul(tensor_stack, tensor_stack.T)
        result_sum = torch.sum(result)
        
        tensor_time = time.time() - start_time
        print(f"Tensor operations completed in {tensor_time:.4f}s")
        print(f"Result: {result_sum.item():.6f}")
    
    # Demo 2: Integration Performance
    print("\nDemo 2: Integration Performance Comparison")
    print("-" * 50)
    
    # Simple ODE: dy/dt = -y, y(0) = 1
    def simple_ode(t, y):
        return -y
    
    rk4 = get_rk4()
    y0 = np.array([1.0])
    t_span = (0.0, 2.0)
    h = 0.01  # Smaller step size for more computation
    
    # CPU integrator
    print("Testing CPU integrator...")
    cpu_integrator = RungeKuttaIntegrator(rk4, use_cuda=False)
    start_time = time.time()
    cpu_result = cpu_integrator.integrate(simple_ode, y0, t_span, h=h)
    cpu_time = time.time() - start_time
    
    print(f"CPU: {cpu_time:.4f}s, {cpu_result.n_steps} steps")
    
    # CUDA integrator
    print("Testing CUDA integrator...")
    cuda_integrator = RungeKuttaIntegrator(rk4, use_cuda=True)
    start_time = time.time()
    cuda_result = cuda_integrator.integrate(simple_ode, y0, t_span, h=h)
    cuda_time = time.time() - start_time
    
    print(f"CUDA: {cuda_time:.4f}s, {cuda_result.n_steps} steps")
    
    # Performance comparison
    if cpu_time > 0 and cuda_time > 0:
        speedup = cpu_time / cuda_time
        print(f"Speedup: {speedup:.2f}x")
        
        # Accuracy comparison
        if cpu_result.success and cuda_result.success:
            error = np.abs(cpu_result.y_vals[-1, 0] - cuda_result.y_vals[-1, 0])
            print(f"Accuracy difference: {error:.2e}")
    
    # Demo 3: Memory Usage
    print("\nDemo 3: CUDA Memory Usage")
    print("-" * 50)
    
    memory_info = ml_pipeline.get_cuda_memory_info()
    if memory_info:
        print(f"Allocated: {memory_info['allocated_gb']:.2f} GB")
        print(f"Cached: {memory_info['cached_gb']:.2f} GB")
        print(f"Free: {memory_info['free_gb']:.2f} GB")
        print(f"Total: {memory_info['total_gb']:.1f} GB")
    
    print("\n‚úÖ CUDA demo completed successfully!")

def demo_batch_processing():
    """Demonstrate batch processing with CUDA."""
    print("\nBatch Processing Demo")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available - skipping batch demo")
        return
    
    config = ModelConfig()
    ml_pipeline = MLPipeline(config)
    
    # Generate larger batch
    print("Generating large batch of tables...")
    batch_sizes = [10, 50, 100, 200]
    
    for batch_size in batch_sizes:
        print(f"\nTesting batch size: {batch_size}")
        
        start_time = time.time()
        tables = ml_pipeline.generator.generate_valid_tables(batch_size, device=str(ml_pipeline.device))
        generation_time = time.time() - start_time
        
        if len(tables) > 0:
            # Convert to tensors
            start_time = time.time()
            tensors = [table.to_tensor(device=ml_pipeline.device) for table in tables]
            tensor_time = time.time() - start_time
            
            # Batch prediction
            start_time = time.time()
            predictions = ml_pipeline.predict_performance(tables)
            prediction_time = time.time() - start_time
            
            print(f"  Generated: {len(tables)} tables in {generation_time:.4f}s")
            print(f"  Tensor conversion: {tensor_time:.4f}s")
            print(f"  Predictions: {prediction_time:.4f}s")
            print(f"  Total: {generation_time + tensor_time + prediction_time:.4f}s")
            
            # Memory usage
            memory_info = ml_pipeline.get_cuda_memory_info()
            if memory_info:
                print(f"  Memory used: {memory_info['allocated_gb']:.2f} GB")

def main():
    """Run CUDA demos."""
    print("CUDA Capabilities Demo")
    print("=" * 60)
    print("This demo showcases the CUDA acceleration features")
    print("of the Novel Numerical Integration Methods project.")
    print()
    
    try:
        demo_cuda_acceleration()
        demo_batch_processing()
        
        print("\n" + "=" * 60)
        print("üéâ All demos completed successfully!")
        print("=" * 60)
        
        if torch.cuda.is_available():
            print("‚úÖ CUDA acceleration is working optimally")
            print("‚úÖ Significant performance improvements achieved")
        else:
            print("‚ö†Ô∏è  CUDA not available - demos ran on CPU")
            print("üí° Install PyTorch with CUDA support for GPU acceleration")
        
    except Exception as e:
        print(f"‚ùå Demo failed with error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
